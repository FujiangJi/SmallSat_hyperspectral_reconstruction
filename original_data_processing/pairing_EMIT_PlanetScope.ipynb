{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a21974-8b3b-4d48-b5bd-92b87253bc4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T20:29:40.522243Z",
     "iopub.status.busy": "2024-10-06T20:29:40.522057Z",
     "iopub.status.idle": "2024-10-06T20:29:41.840809Z",
     "shell.execute_reply": "2024-10-06T20:29:41.840491Z",
     "shell.execute_reply.started": "2024-10-06T20:29:40.522220Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import psutil\n",
    "import xarray as xr\n",
    "import joblib\n",
    "import logging\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from scipy import stats\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from osgeo import gdal, ogr,gdalconst\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from shapely.geometry import Point, Polygon,box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efc9170-5fae-4718-b709-e58103f28863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T20:29:41.842695Z",
     "iopub.status.busy": "2024-10-06T20:29:41.842586Z",
     "iopub.status.idle": "2024-10-06T20:29:41.856509Z",
     "shell.execute_reply": "2024-10-06T20:29:41.856224Z",
     "shell.execute_reply.started": "2024-10-06T20:29:41.842685Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_tif(tif_file):\n",
    "    dataset = gdal.Open(tif_file)\n",
    "    cols = dataset.RasterXSize\n",
    "    rows = dataset.RasterYSize\n",
    "    im_proj = (dataset.GetProjection())\n",
    "    im_Geotrans = (dataset.GetGeoTransform())\n",
    "    im_data = dataset.ReadAsArray(0, 0, cols, rows)\n",
    "    if im_data.ndim == 3:\n",
    "        im_data = np.moveaxis(dataset.ReadAsArray(0, 0, cols, rows), 0, -1)\n",
    "    return im_data, im_Geotrans, im_proj,rows, cols\n",
    "def array_to_geotiff(array, output_path, geo_transform, projection, band_names=None):\n",
    "    rows, cols, num_bands = array.shape\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    dataset = driver.Create(output_path, cols, rows, num_bands, gdal.GDT_Float32)\n",
    "    \n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    \n",
    "    for band_num in range(num_bands):\n",
    "        band = dataset.GetRasterBand(band_num + 1)\n",
    "        band.WriteArray(array[:, :, band_num])\n",
    "        band.FlushCache()\n",
    "        \n",
    "        if band_names:\n",
    "            band.SetDescription(band_names[band_num])\n",
    "    \n",
    "    dataset = None\n",
    "    band = None\n",
    "\n",
    "class BandInfo:\n",
    "    def __init__(self):\n",
    "        self.centers = None\n",
    "        self.bandwidths = None\n",
    "        self.centers_stdevs = None\n",
    "        self.bandwidth_stdevs = None\n",
    "        self.band_quantity = None\n",
    "        self.band_unit = None\n",
    "\n",
    "def erf_local(x):\n",
    "    sign = 1 if x >= 0 else -1\n",
    "    x = abs(x)\n",
    "    a1 =  0.254829592\n",
    "    a2 = -0.284496736\n",
    "    a3 =  1.421413741\n",
    "    a4 = -1.453152027\n",
    "    a5 =  1.061405429\n",
    "    p  =  0.3275911\n",
    "\n",
    "    t = 1.0/(1.0 + p*x)\n",
    "    y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*math.exp(-x*x)\n",
    "    return sign*y # erf(-x) = -erf(x)\n",
    "\n",
    "try:\n",
    "    from math import erf\n",
    "except:\n",
    "    try:\n",
    "        from scipy.special import erf\n",
    "    except:\n",
    "        erf = erf_local\n",
    "\n",
    "def erfc(z):\n",
    "    '''Complement of the error function.'''\n",
    "    return 1.0 - erf(z)\n",
    "\n",
    "def normal_cdf(x):\n",
    "    '''CDF of the normal distribution.'''\n",
    "    sqrt2 = 1.4142135623730951\n",
    "    return 0.5 * erfc(-x / sqrt2)\n",
    "\n",
    "def normal_integral(a, b):\n",
    "    '''Integral of the normal distribution from a to b.'''\n",
    "    return normal_cdf(b) - normal_cdf(a)\n",
    "\n",
    "def ranges_overlap(R1, R2):\n",
    "    '''Returns True if there is overlap between ranges of pairs R1 and R2.'''\n",
    "    if (R1[0] < R2[0] and R1[1] < R2[0]) or \\\n",
    "       (R1[0] > R2[1] and R1[1] > R2[1]):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def overlap(R1, R2):\n",
    "    '''Returns (min, max) of overlap between the ranges of pairs R1 and R2.'''\n",
    "    return (max(R1[0], R2[0]), min(R1[1], R2[1]))\n",
    "\n",
    "def normal(mean, stdev, x):\n",
    "    sqrt_2pi = 2.5066282746310002\n",
    "    return math.exp(-((x - mean) / stdev)**2 / 2.0) / (sqrt_2pi * stdev)\n",
    "\n",
    "def build_fwhm(centers):\n",
    "    '''Returns FWHM list, assuming FWHM is midway between adjacent bands.\n",
    "    '''\n",
    "    fwhm = [0] * len(centers)\n",
    "    fwhm[0] = centers[1] - centers[0]\n",
    "    fwhm[-1] = centers[-1] - centers[-2]\n",
    "    for i in range(1, len(centers) - 1):\n",
    "        fwhm[i] = (centers[i + 1] - centers[i - 1]) / 2.0\n",
    "    return fwhm\n",
    "\n",
    "def create_resampling_matrix(centers1, fwhm1, centers2, fwhm2):\n",
    "    logger = logging.getLogger('spectral')\n",
    "\n",
    "    sqrt_8log2 = 2.3548200450309493\n",
    "\n",
    "    N1 = len(centers1)\n",
    "    N2 = len(centers2)\n",
    "    bounds1 = [[centers1[i] - fwhm1[i] / 2.0, centers1[i] + fwhm1[i] /\n",
    "                2.0] for i in range(N1)]\n",
    "    bounds2 = [[centers2[i] - fwhm2[i] / 2.0, centers2[i] + fwhm2[i] /\n",
    "                2.0] for i in range(N2)]\n",
    "\n",
    "    M = np.zeros([N2, N1])\n",
    "\n",
    "    jStart = 0\n",
    "    nan = float('nan')\n",
    "    for i in range(N2):\n",
    "        stdev = fwhm2[i] / sqrt_8log2\n",
    "        j = jStart\n",
    "\n",
    "        # Find the first original band that overlaps the new band\n",
    "        while j < N1 and bounds1[j][1] < bounds2[i][0]:\n",
    "            j += 1\n",
    "\n",
    "        if j == N1:\n",
    "            logger.info(('No overlap for target band %d (%f / %f)' % (\n",
    "                i, centers2[i], fwhm2[i])))\n",
    "            M[i, 0] = nan\n",
    "            continue\n",
    "\n",
    "        matches = []\n",
    "\n",
    "        # Get indices for all original bands that overlap the new band\n",
    "        while j < N1 and bounds1[j][0] < bounds2[i][1]:\n",
    "            if ranges_overlap(bounds1[j], bounds2[i]):\n",
    "                matches.append(j)\n",
    "            j += 1\n",
    "\n",
    "        # Put NaN in first element of any row that doesn't produce a band in\n",
    "        # the new schema.\n",
    "        if len(matches) == 0:\n",
    "            logger.info('No overlap for target band %d (%f / %f)',\n",
    "                         i, centers2[i], fwhm2[i])\n",
    "            M[i, 0] = nan\n",
    "            continue\n",
    "\n",
    "        # Determine the weights for the original bands that overlap the new\n",
    "        # band. There may be multiple bands that overlap or even just a single\n",
    "        # band that only partially overlaps.  Weights are normoalized so either\n",
    "        # case can be handled.\n",
    "\n",
    "        overlaps = [overlap(bounds1[k], bounds2[i]) for k in matches]\n",
    "        contribs = np.zeros(len(matches))\n",
    "        A = 0.\n",
    "        for k in range(len(matches)):\n",
    "            #endNorms = [normal(centers2[i], stdev, x) for x in overlaps[k]]\n",
    "            #dA = (overlaps[k][1] - overlaps[k][0]) * sum(endNorms) / 2.0\n",
    "            (a, b) = [(x - centers2[i]) / stdev for x in overlaps[k]]\n",
    "            dA = normal_integral(a, b)\n",
    "            contribs[k] = dA\n",
    "            A += dA\n",
    "        contribs = contribs / A\n",
    "        for k in range(len(matches)):\n",
    "            M[i, matches[k]] = contribs[k]\n",
    "    return M\n",
    "\n",
    "class BandResampler:\n",
    "    def __init__(self, centers1, centers2, fwhm1=None, fwhm2=None):\n",
    "        if isinstance(centers1, BandInfo):\n",
    "            fwhm1 = centers1.bandwidths\n",
    "            centers1 = centers1.centers\n",
    "        if isinstance(centers2, BandInfo):\n",
    "            fwhm2 = centers2.bandwidths\n",
    "            centers2 = centers2.centers\n",
    "        if fwhm1 is None:\n",
    "            fwhm1 = build_fwhm(centers1)\n",
    "        if fwhm2 is None:\n",
    "            fwhm2 = build_fwhm(centers2)\n",
    "        self.matrix = create_resampling_matrix(\n",
    "            centers1, fwhm1, centers2, fwhm2)\n",
    "\n",
    "    def __call__(self, spectrum):\n",
    "        return np.dot(self.matrix, spectrum)\n",
    "def do_resample(spectra, source_wvl, source_fwhm):\n",
    "    centers1 = source_wvl\n",
    "    centers2 = np.arange(400,1001,1)\n",
    "    fwhm1 = source_fwhm\n",
    "    fwhm2 = build_fwhm(centers2)\n",
    "    resampler = BandResampler(centers1, centers2, fwhm1, fwhm2)\n",
    "    resampled_spectra = resampler(spectra)\n",
    "    return resampled_spectra\n",
    "\n",
    "def compute_weighted_reflectance(response_curve_wl, hyper_wl, hyper_data, response_curve, hyper_fwhm):\n",
    "    interp_reflectance = do_resample(hyper_data, hyper_wl, hyper_fwhm)    \n",
    "    numerator = np.trapz(interp_reflectance * response_curve, response_curve_wl)\n",
    "    denominator = np.trapz(response_curve, response_curve_wl)\n",
    "    weighted_reflectance = numerator/denominator\n",
    "    return weighted_reflectance\n",
    "\n",
    "def run_parallel(image_chunk, df, wvl, fwhm):\n",
    "    results = np.zeros(shape = (image_chunk.shape[0], image_chunk.shape[1],8))\n",
    "    for i in range(image_chunk.shape[0]):\n",
    "        for j in range(image_chunk.shape[1]):\n",
    "            ratio_data = image_chunk[i, j, :]\n",
    "            response_curve_wl = df[df.columns[0]]\n",
    "            simulated_nosie = []\n",
    "            for kk in range(1,9):\n",
    "                response_curve = df[df.columns[kk]]\n",
    "                noise = compute_weighted_reflectance(response_curve_wl, wvl, ratio_data, response_curve, fwhm)\n",
    "                simulated_nosie.append(noise)\n",
    "            results[i,j,:] = simulated_nosie\n",
    "    return results\n",
    "\n",
    "def raster_to_points(geotiff, shp_name):\n",
    "    inDs = gdal.Open(geotiff)\n",
    "    DsoutDs = gdal.Translate(f\"{shp_name}.xyz\", inDs, format='XYZ', creationOptions=[\"ADD_HEADER_LINE=YES\"])\n",
    "    outDs = None\n",
    "    try:\n",
    "        os.remove(f'{shp_name}.csv')\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    os.rename(f'{shp_name}.xyz', f'{shp_name}.csv')\n",
    "    os.system('ogr2ogr -f \"ESRI Shapefile\" -oo X_POSSIBLE_NAMES=X* -oo Y_POSSIBLE_NAMES=Y* -oo KEEP_GEOM_COLUMNS=NO {0}.shp {0}.csv'.format(shp_name))\n",
    "    try:\n",
    "        os.remove(f'{shp_name}.csv')\n",
    "    except OSError:\n",
    "        pass\n",
    "    crs_wkt = inDs.GetProjection()\n",
    "    shp_layer = gpd.read_file(f\"{shp_name}.shp\")\n",
    "    shp_layer.crs = crs_wkt\n",
    "    shp_layer.to_file(f\"{shp_name}.shp\")\n",
    "    return\n",
    "\n",
    "def filter_points(gdf, min_distance):\n",
    "    gdf_copy = gdf.copy()\n",
    "    to_remove = []\n",
    "\n",
    "    for index, row in gdf_copy.iterrows():\n",
    "        if index not in to_remove:\n",
    "            distances = gdf_copy.geometry.distance(row.geometry)\n",
    "            close_points = distances[distances < min_distance].index.tolist()\n",
    "            close_points.remove(index)\n",
    "            to_remove.extend(close_points)\n",
    "\n",
    "    gdf_copy.drop(index=to_remove, inplace=True)\n",
    "    gdf_copy.reset_index(drop = True, inplace = True)\n",
    "    return gdf_copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f1aa40c-18f9-49e1-90bc-51aec3b36f9c",
   "metadata": {},
   "source": [
    "## 1. process data of 20230422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e636a5-9980-4814-831b-495b052d824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/2_processed_data/1_SHIFT_areas/\"\n",
    "out_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "\n",
    "file_name = \"EMIT_L2A_RFL_001_20230422T195924_2311213_002_reflectance.img\"\n",
    "src_ds = gdal.Open(f\"{emit_path}{file_name}\")\n",
    "out_ds = f\"{out_path}EMIT_L2A_RFL_20230422.tif\"\n",
    "gdal.Translate(out_ds, src_ds, format='GTiff')\n",
    "src_ds = None\n",
    "\n",
    "file_name = \"EMIT_L2A_RFLUNCERT_001_20230422T195924_2311213_002_reflectance_uncertainty.img\"\n",
    "src_ds = gdal.Open(f\"{emit_path}{file_name}\")\n",
    "out_ds = f\"{out_path}EMIT_L2A_RFLUNCERT_20230422.tif\"\n",
    "gdal.Translate(out_ds, src_ds, format='GTiff')\n",
    "src_ds = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc66e210-836a-4d2a-aadc-c6bb4b1799c0",
   "metadata": {},
   "source": [
    "### project EMIT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68c8dfc6-5272-4a83-9ecb-0ff3020b95df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T00:38:45.979675Z",
     "iopub.status.busy": "2024-09-27T00:38:45.978640Z",
     "iopub.status.idle": "2024-09-27T00:40:25.828205Z",
     "shell.execute_reply": "2024-09-27T00:40:25.826606Z",
     "shell.execute_reply.started": "2024-09-27T00:38:45.979607Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "\n",
    "emit_file = \"EMIT_L2A_RFL_20230422.tif\"\n",
    "in_tif = f\"{data_path}/{emit_file}\"\n",
    "out_tif = f\"{data_path}/{emit_file[:-4]}_projection.tif\"\n",
    "input_ds = gdal.Open(in_tif)\n",
    "output_ds = gdal.Warp(out_tif, input_ds, dstSRS='EPSG:32611')\n",
    "\n",
    "emit_file = \"EMIT_L2A_RFLUNCERT_20230422.tif\"\n",
    "in_tif = f\"{data_path}/{emit_file}\"\n",
    "out_tif = f\"{data_path}/{emit_file[:-4]}_projection.tif\"\n",
    "input_ds = gdal.Open(in_tif)\n",
    "output_ds = gdal.Warp(out_tif, input_ds, dstSRS='EPSG:32611')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e83b5135-96b4-433a-ba33-2e3c87cb9071",
   "metadata": {},
   "source": [
    "### clip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eab98d8f-5ef7-46b3-a3e1-c03a2b18c93c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T00:40:43.549895Z",
     "iopub.status.busy": "2024-09-27T00:40:43.548859Z",
     "iopub.status.idle": "2024-09-27T00:41:21.310789Z",
     "shell.execute_reply": "2024-09-27T00:41:21.309616Z",
     "shell.execute_reply.started": "2024-09-27T00:40:43.549824Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "shp = f\"{data_path}0_clip_shp/0_first_beginning_clip_shp.shp\"\n",
    "emit_file = \"EMIT_L2A_RFL_20230422_projection.tif\"\n",
    "planet_file = \"PlanetScope_RFL_20230422.tif\"\n",
    "emit_file2 = \"EMIT_L2A_RFLUNCERT_20230422_projection.tif\"\n",
    "\n",
    "gdf = gpd.read_file(shp)\n",
    "bounds = gdf.bounds\n",
    "min_x = bounds[\"minx\"].values[0]\n",
    "min_y = bounds[\"miny\"].values[0]\n",
    "max_x = bounds[\"maxx\"].values[0]\n",
    "max_y = bounds[\"maxy\"].values[0]\n",
    "ul_x, ul_y = (min_x, max_y)\n",
    "lr_x, lr_y = (max_x, min_y)\n",
    "\n",
    "### clip EMIT data\n",
    "input_tif = f\"{data_path}/{emit_file}\"\n",
    "output_tif = f\"{data_path}/{emit_file[:-4]}_clipped.tif\"\n",
    "gdal.Warp(output_tif, input_tif, format = 'GTiff', outputBounds=(ul_x, lr_y, lr_x, ul_y))\n",
    "output_tif = None\n",
    "\n",
    "### clip Planet data\n",
    "input_tif = f\"{data_path}/{planet_file}\"\n",
    "output_tif = f\"{data_path}/{planet_file[:-4]}_clipped.tif\"\n",
    "gdal.Warp(output_tif, input_tif, format = 'GTiff', outputBounds=(ul_x, lr_y, lr_x, ul_y))\n",
    "output_tif = None\n",
    "\n",
    "\n",
    "### clip EMIT uncertainty data\n",
    "input_tif = f\"{data_path}/{emit_file2}\"\n",
    "output_tif = f\"{data_path}/{emit_file2[:-4]}_clipped.tif\"\n",
    "gdal.Warp(output_tif, input_tif, format = 'GTiff', outputBounds=(ul_x, lr_y, lr_x, ul_y))\n",
    "output_tif = None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7f37df6-60dd-4060-97da-6f89c26b15d4",
   "metadata": {},
   "source": [
    "### geo-corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d7208-9913-4742-a6c3-76010c68296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the upscaled Planet data as reference to correct EMIT data.\n",
    "# data_path = \"/Volumes/ChenLab-1/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "# emit_file = \"EMIT_L2A_RFL_20230422_projection_clipped.tif\"\n",
    "\n",
    "# emit_data, emit_Geotrans, emit_proj,_, _ = read_tif(f\"{data_path}{emit_file}\")\n",
    "# nir = emit_data[:,:,51]\n",
    "# red = emit_data[:,:,39]\n",
    "# green = emit_data[:,:,23]\n",
    "\n",
    "# data = [nir, red, green]\n",
    "# data = np.stack(data, axis = 2)\n",
    "\n",
    "# output_path = f\"{data_path}0_geocorrections/EMIT_L2A_RFL_20230422_3bands.tif\"\n",
    "# array_to_geotiff(data, output_path, emit_Geotrans, emit_proj, band_names=[\"nir\", \"red\", \"green\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75331ed1-c56c-4970-96ee-87e496008ece",
   "metadata": {},
   "source": [
    "### clip again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "420416ab-b9cb-498c-9c07-5ecbcc002e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T00:44:08.511645Z",
     "iopub.status.busy": "2024-09-27T00:44:08.510704Z",
     "iopub.status.idle": "2024-09-27T00:44:20.541395Z",
     "shell.execute_reply": "2024-09-27T00:44:20.541050Z",
     "shell.execute_reply.started": "2024-09-27T00:44:08.511579Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "shp = f\"{data_path}0_clip_shp/20230422_clip_shp.shp\"\n",
    "emit_file = \"EMIT_L2A_RFL_20230422_projection_clipped_modified.tif\"\n",
    "planet_file = \"PlanetScope_RFL_20230422_clipped.tif\"\n",
    "emit_file2 = \"EMIT_L2A_RFLUNCERT_20230422_projection_clipped_modified.tif\"\n",
    "\n",
    "gdf = gpd.read_file(shp)\n",
    "bounds = gdf.bounds\n",
    "min_x = bounds[\"minx\"].values[0]\n",
    "min_y = bounds[\"miny\"].values[0]\n",
    "max_x = bounds[\"maxx\"].values[0]\n",
    "max_y = bounds[\"maxy\"].values[0]\n",
    "ul_x, ul_y = (min_x, max_y)\n",
    "lr_x, lr_y = (max_x, min_y)\n",
    "\n",
    "# ### clip EMIT data\n",
    "# input_tif = f\"{data_path}/{emit_file}\"\n",
    "# output_tif = f\"{data_path}/{emit_file[:-4]}_clipped.tif\"\n",
    "# gdal.Warp(output_tif, input_tif, format = 'GTiff', outputBounds=(ul_x, lr_y, lr_x, ul_y))\n",
    "# output_tif = None\n",
    "\n",
    "# ### clip Planet data\n",
    "# input_tif = f\"{data_path}/{planet_file}\"\n",
    "# output_tif = f\"{data_path}/{planet_file[:-4]}_clipped.tif\"\n",
    "# gdal.Warp(output_tif, input_tif, format = 'GTiff', outputBounds=(ul_x, lr_y, lr_x, ul_y))\n",
    "# output_tif = None\n",
    "\n",
    "\n",
    "### clip EMIT uncertainty data\n",
    "input_tif = f\"{data_path}/{emit_file2}\"\n",
    "output_tif = f\"{data_path}/{emit_file2[:-4]}_clipped.tif\"\n",
    "gdal.Warp(output_tif, input_tif, format = 'GTiff', outputBounds=(ul_x, lr_y, lr_x, ul_y))\n",
    "output_tif = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78e2242c-e53f-4f6e-a170-2d147929340f",
   "metadata": {},
   "source": [
    "### process the bad bands of EMIT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d13836c-8de0-4fda-8535-03fc76d1dc74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T20:29:46.908580Z",
     "iopub.status.busy": "2024-10-06T20:29:46.907573Z",
     "iopub.status.idle": "2024-10-06T20:29:47.085633Z",
     "shell.execute_reply": "2024-10-06T20:29:47.085332Z",
     "shell.execute_reply.started": "2024-10-06T20:29:46.908528Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/1_original_data/1_SHIFT_areas/1_Spaceborne_hyperspectral_imagery/\"\n",
    "file_name = \"EMIT_L2A_RFL_001_20230422T195924_2311213_002.nc\"\n",
    "ds_nc = xr.open_dataset(f\"{data_path}{file_name}\", engine=\"h5netcdf\", group=\"sensor_band_parameters\")\n",
    "fwhm = ds_nc[\"fwhm\"].values\n",
    "wvl = ds_nc[\"wavelengths\"].values\n",
    "good_wvl = ds_nc[\"good_wavelengths\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c508d34-dee9-4301-844a-29a0b60b36f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T20:31:54.316059Z",
     "iopub.status.busy": "2024-10-06T20:31:54.315564Z",
     "iopub.status.idle": "2024-10-06T20:32:59.079927Z",
     "shell.execute_reply": "2024-10-06T20:32:59.078244Z",
     "shell.execute_reply.started": "2024-10-06T20:31:54.316025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(994, 340, 285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Chunks:   0%|                                                                           | 0/140 [00:00<?, ?it/s]\u001b[A\n",
      "Processing Chunks:  17%|███████████▏                                                     | 24/140 [00:00<00:00, 121.66it/s]\u001b[A\n",
      "Processing Chunks:  34%|██████████████████████▋                                           | 48/140 [00:02<00:06, 14.74it/s]\u001b[A\n",
      "Processing Chunks:  51%|█████████████████████████████████▉                                | 72/140 [00:03<00:02, 22.87it/s]\u001b[A\n",
      "Processing Chunks:  69%|█████████████████████████████████████████████▎                    | 96/140 [00:03<00:01, 33.33it/s]\u001b[A\n",
      "Processing Chunks: 100%|█████████████████████████████████████████████████████████████████| 140/140 [00:03<00:00, 38.96it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(994, 340, 285)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "emit_file = \"EMIT_L2A_RFL_20230422_projection_clipped_modified_clipped.tif\"\n",
    "\n",
    "emit_data, emit_Geotrans, emit_proj, _ , _ = read_tif(f\"{data_path}{emit_file}\")\n",
    "print(emit_data.shape)\n",
    "\n",
    "def EMIT_bad_bands_parallel(img_chunk, good_wvl_mask):\n",
    "    small_image = img_chunk.copy()\n",
    "    results = np.zeros(shape = (small_image.shape[0], small_image.shape[1],small_image.shape[2]))\n",
    "    for i in range(small_image.shape[0]):\n",
    "        for j in range(small_image.shape[1]):\n",
    "            spectra_data = small_image[i, j, :]\n",
    "            \n",
    "            valid_indices = np.where(spectra_data >= 0)[0]\n",
    "            invalid_indices = np.where(spectra_data < 0)[0]\n",
    "            if len(valid_indices) > 0 and len(invalid_indices) > 0:\n",
    "                spectra_data[invalid_indices] = np.interp(invalid_indices, valid_indices, spectra_data[valid_indices])\n",
    "            results[i,j] = spectra_data\n",
    "    mask = good_wvl_mask[np.newaxis, np.newaxis, :] \n",
    "    results = np.where(mask == 0, 0, results)\n",
    "    return results\n",
    "\n",
    "chunk_size = 50\n",
    "image_chunks = []\n",
    "for i in range(0, emit_data.shape[0], chunk_size):\n",
    "        for j in range(0, emit_data.shape[1], chunk_size):\n",
    "            chunk = emit_data[i:i + chunk_size, j:j + chunk_size]\n",
    "            image_chunks.append(chunk)\n",
    "\n",
    "num_processes = psutil.cpu_count(logical=False)\n",
    "chunk_results = Parallel(n_jobs=num_processes)(delayed(EMIT_bad_bands_parallel)(image_chunk,good_wvl) for image_chunk in tqdm(image_chunks,desc=\"Processing Chunks\"))\n",
    "\n",
    "final_imagery = np.zeros(shape = (emit_data.shape[0], emit_data.shape[1], emit_data.shape[2]))\n",
    "chunk_index = 0\n",
    "for i in range(0, final_imagery.shape[0], chunk_size):\n",
    "    for j in range(0, final_imagery.shape[1], chunk_size):\n",
    "        final_imagery[i:i + chunk_size, j:j + chunk_size,:] = chunk_results[chunk_index]\n",
    "        chunk_index = chunk_index+1\n",
    "print(final_imagery.shape)\n",
    "\n",
    "out_tif = f\"{data_path}{emit_file[:-4]}_interp.tif\"\n",
    "band_names = [f\"{x} nm\" for x in wvl]\n",
    "array_to_geotiff(final_imagery, out_tif, emit_Geotrans, emit_proj, band_names=band_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "910c4078-0bbb-4451-b628-b689fe3d8216",
   "metadata": {},
   "source": [
    "### scale factors to convert PlanetScope reflectance to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ac46101-44f9-41ed-aee9-435312aa1897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T23:25:35.008030Z",
     "iopub.status.busy": "2024-09-26T23:25:35.006964Z",
     "iopub.status.idle": "2024-09-26T23:28:56.993455Z",
     "shell.execute_reply": "2024-09-26T23:28:56.992227Z",
     "shell.execute_reply.started": "2024-09-26T23:25:35.007927Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab-1/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "planet_file = \"PlanetScope_RFL_20230422_clipped_clipped.tif\"\n",
    "im_data, im_Geotrans, im_proj,rows, cols = read_tif(f\"{data_path}{planet_file}\")\n",
    "im_data = im_data/10000\n",
    "\n",
    "dataset = gdal.Open(f\"{data_path}{planet_file}\") \n",
    "num_bands = dataset.RasterCount\n",
    "band_names = []\n",
    "for band_index in range(1, num_bands + 1):\n",
    "    band = dataset.GetRasterBand(band_index)\n",
    "    band_name = band.GetDescription() \n",
    "    band_names.append(band_name)\n",
    "    \n",
    "output_path = f\"{data_path}{planet_file[:-4]}_scaled.tif\"\n",
    "array_to_geotiff(im_data, output_path, im_Geotrans, im_proj, band_names=band_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f622850-ce68-4999-a262-3a882ecf7d33",
   "metadata": {},
   "source": [
    "### upscale PlanetScope to the Spatial resolution of EMIT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8eeca48-3a01-4118-a072-944dc0cdef2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T23:32:04.099353Z",
     "iopub.status.busy": "2024-09-26T23:32:04.098710Z",
     "iopub.status.idle": "2024-09-26T23:32:08.083486Z",
     "shell.execute_reply": "2024-09-26T23:32:08.083074Z",
     "shell.execute_reply.started": "2024-09-26T23:32:04.099304Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab-1/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "planet_file = \"PlanetScope_RFL_20230422_clipped_clipped_scaled.tif\"\n",
    "emit_file = \"EMIT_L2A_RFL_20230422_projection_clipped_modified_clipped.tif\"\n",
    "\n",
    "emit_data, emit_Geotrans, emit_proj,_, _ = read_tif(f\"{data_path}{emit_file}\")\n",
    "x_res = emit_Geotrans[1]\n",
    "y_res = abs(emit_Geotrans[5])\n",
    "\n",
    "input_ds = gdal.Open(f\"{data_path}{planet_file}\")\n",
    "output_path = f\"{data_path}{planet_file[:-4]}_aggregated_60m.tif\"\n",
    "\n",
    "gdal.Warp(output_path, input_ds, xRes=x_res, yRes=y_res,resampleAlg=gdalconst.GRA_Bilinear)\n",
    "    \n",
    "input_ds = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f61f8c3-abcd-4266-bf52-74915cb78050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T22:38:41.361092Z",
     "iopub.status.busy": "2024-09-26T22:38:41.360652Z",
     "iopub.status.idle": "2024-09-26T22:38:41.830626Z",
     "shell.execute_reply": "2024-09-26T22:38:41.830222Z",
     "shell.execute_reply.started": "2024-09-26T22:38:41.361066Z"
    }
   },
   "source": [
    "### calculate the EMIT noise ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a69fa5c2-d59d-44b2-814c-08d1125dc299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T02:39:12.621788Z",
     "iopub.status.busy": "2024-10-01T02:39:12.621280Z",
     "iopub.status.idle": "2024-10-01T02:39:12.680805Z",
     "shell.execute_reply": "2024-10-01T02:39:12.679948Z",
     "shell.execute_reply.started": "2024-10-01T02:39:12.621741Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/1_original_data/1_SHIFT_areas/1_Spaceborne_hyperspectral_imagery/\"\n",
    "file_name = \"EMIT_L2A_RFL_001_20230422T195924_2311213_002.nc\"\n",
    "ds_nc = xr.open_dataset(f\"{data_path}{file_name}\", engine=\"h5netcdf\", group=\"sensor_band_parameters\")\n",
    "fwhm = ds_nc[\"fwhm\"].values\n",
    "wvl = ds_nc[\"wavelengths\"].values\n",
    "good_wvl = ds_nc[\"good_wavelengths\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4151b44-37dd-4884-a715-a4818d1d35c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T18:39:48.440412Z",
     "iopub.status.busy": "2024-09-30T18:39:48.439656Z",
     "iopub.status.idle": "2024-09-30T18:41:01.855410Z",
     "shell.execute_reply": "2024-09-30T18:41:01.854446Z",
     "shell.execute_reply.started": "2024-09-30T18:39:48.440352Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "emit_file = \"EMIT_L2A_RFL_20230422_projection_clipped_modified_clipped.tif\"\n",
    "emit_uncertainty = \"EMIT_L2A_RFLUNCERT_20230422_projection_clipped_modified_clipped.tif\"\n",
    "\n",
    "emit_data, emit_Geotrans, emit_proj, _ , _ = read_tif(f\"{data_path}{emit_file}\")\n",
    "uncert_data, uncert_Geotrans, uncert_proj, _ , _ = read_tif(f\"{data_path}{emit_uncertainty}\")\n",
    "ratio = uncert_data/emit_data\n",
    "\n",
    "out_tif = f\"{data_path}EMIT_reflectance_uncertainty_ratio.tif\"\n",
    "band_names = [f\"{x}nm\" for x in wvl]\n",
    "array_to_geotiff(ratio, out_tif, emit_Geotrans, emit_proj, band_names=band_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78a93dc3-b196-48b9-8a26-d152b8eeb52a",
   "metadata": {},
   "source": [
    "### add EMIT noise to upscaled PlanetScope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83d7889d-ae10-4e34-a6bb-442d8098c98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T01:50:41.316120Z",
     "iopub.status.busy": "2024-10-01T01:50:41.315682Z",
     "iopub.status.idle": "2024-10-01T02:19:45.778692Z",
     "shell.execute_reply": "2024-10-01T02:19:45.777614Z",
     "shell.execute_reply.started": "2024-10-01T01:50:41.316095Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Chunks:   0%|                                                | 0/140 [00:00<?, ?it/s]\u001b[A\n",
      "Processing Chunks:  17%|██████▋                                | 24/140 [00:02<00:11, 10.54it/s]\u001b[A\n",
      "Processing Chunks:  19%|███████▏                               | 26/140 [00:02<00:11,  9.93it/s]\u001b[A\n",
      "Processing Chunks:  19%|███████▌                               | 27/140 [00:02<00:11,  9.83it/s]\u001b[A\n",
      "Processing Chunks:  34%|█████████████▎                         | 48/140 [04:36<13:37,  8.88s/it]\u001b[A\n",
      "Processing Chunks:  51%|████████████████████                   | 72/140 [09:39<12:17, 10.85s/it]\u001b[A\n",
      "Processing Chunks:  52%|████████████████████▎                  | 73/140 [09:39<11:45, 10.53s/it]\u001b[A\n",
      "Processing Chunks:  69%|██████████████████████████▋            | 96/140 [13:44<07:45, 10.59s/it]\u001b[A\n",
      "Processing Chunks: 100%|██████████████████████████████████████| 140/140 [19:10<00:00,  8.22s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "spectral_response = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/1_support_materials/Spectral_response_curve_PlanetScope.csv\"\n",
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "emit_noise_file = \"EMIT_reflectance_uncertainty_ratio.tif\"\n",
    "planet_file = \"PlanetScope_RFL_20230422_clipped_clipped_scaled_aggregated_60m.tif\"\n",
    "df = pd.read_csv(spectral_response)\n",
    "\n",
    "emit_noise, emit_Geotrans, emit_proj, _ , _ = read_tif(f\"{data_path}{emit_noise_file}\")\n",
    "planet_data, planet_Geotrans, planet_proj,_, _ = read_tif(f\"{data_path}{planet_file}\")\n",
    "\n",
    "chunk_size = 50\n",
    "image_chunks = []\n",
    "for i in range(0, emit_noise.shape[0], chunk_size):\n",
    "        for j in range(0, emit_noise.shape[1], chunk_size):\n",
    "            chunk = emit_noise[i:i + chunk_size, j:j + chunk_size]\n",
    "            image_chunks.append(chunk)\n",
    "\n",
    "num_processes = psutil.cpu_count(logical=False)\n",
    "chunk_results = Parallel(n_jobs=num_processes)(delayed(run_parallel)(image_chunk, df, wvl, fwhm) for image_chunk in tqdm(image_chunks,desc=\"Processing Chunks\"))\n",
    "\n",
    "convolved_noise_ratio = np.zeros(shape = (emit_noise.shape[0], emit_noise.shape[1], 8))\n",
    "chunk_index = 0\n",
    "for i in range(0, convolved_noise_ratio.shape[0], chunk_size):\n",
    "    for j in range(0, convolved_noise_ratio.shape[1], chunk_size):\n",
    "        convolved_noise_ratio[i:i + chunk_size, j:j + chunk_size,:] = chunk_results[chunk_index]\n",
    "        chunk_index = chunk_index+1\n",
    "\n",
    "planet_uncertainty = planet_data*convolved_noise_ratio\n",
    "gaussian_noise = np.random.normal(0, 1, planet_uncertainty.shape) * planet_uncertainty\n",
    "planet_noisy_array = planet_data + gaussian_noise\n",
    "\n",
    "dataset = gdal.Open(f\"{data_path}{planet_file}\") \n",
    "num_bands = dataset.RasterCount\n",
    "band_names = []\n",
    "for band_index in range(1, num_bands + 1):\n",
    "    band = dataset.GetRasterBand(band_index)\n",
    "    band_name = band.GetDescription() \n",
    "    band_names.append(band_name)\n",
    "\n",
    "out_tif = f\"{data_path}{planet_file[:-4]}_noisy.tif\"\n",
    "array_to_geotiff(planet_noisy_array, out_tif, planet_Geotrans, planet_proj, band_names=band_names)\n",
    "out_tif = f\"{data_path}Planet_uncertainty_ratio_from_convolved_EMIT_ratio.tif\"\n",
    "array_to_geotiff(convolved_noise_ratio, out_tif, planet_Geotrans, planet_proj, band_names=band_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b504b6c-02f1-4441-9fb2-34bf20e922f4",
   "metadata": {},
   "source": [
    "### inter-calibration for EMIT and PlanetScope imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559fe398-fe16-4255-ac11-4c129858a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "out_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/2_extracted_points_for_intercalibration/\"\n",
    "file_name = \"EMIT_L2A_RFL_20230422_projection_clipped_modified_clipped.tif\"\n",
    "\n",
    "raster_to_points(f\"{data_path}{file_name}\", f\"{out_path}extracted_points\")\n",
    "\n",
    "points = gpd.read_file(f'{out_path}extracted_points.shp')\n",
    "filtered_points = filter_points(points, 150)\n",
    "filtered_points.to_file(f'{out_path}extracted_points_filtered.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2927b5e2-c1fd-4ee5-a834-bf85a12f60dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T02:55:09.064134Z",
     "iopub.status.busy": "2024-10-01T02:55:09.063651Z",
     "iopub.status.idle": "2024-10-01T02:55:30.893919Z",
     "shell.execute_reply": "2024-10-01T02:55:30.892906Z",
     "shell.execute_reply.started": "2024-10-01T02:55:09.064102Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "point_path = f\"{data_path}2_extracted_points_for_intercalibration/\"\n",
    "out_path = f\"{data_path}3_intercalibration_training_data/\"\n",
    "\n",
    "emit_file = \"EMIT_L2A_RFL_20230422_projection_clipped_modified_clipped_interp.tif\"\n",
    "planet_file = \"PlanetScope_RFL_20230422_clipped_clipped_scaled_aggregated_60m_noisy.tif\"\n",
    "planet_file2 = \"PlanetScope_RFL_20230422_clipped_clipped_scaled_aggregated_60m.tif\"\n",
    "shp_name = \"extracted_points_filtered.shp\"\n",
    "\n",
    "points = gpd.read_file(f\"{point_path}{shp_name}\")\n",
    "points.drop(columns=['Z'],inplace = True)\n",
    "\n",
    "tiff_ds = gdal.Open(f\"{data_path}{emit_file}\")\n",
    "num_bands = tiff_ds.RasterCount\n",
    "band_names = wvl.tolist()\n",
    "\n",
    "extracted_bands = band_names[:100]\n",
    "extracted_bands = [f\"{round(x, 5)} nm\" for x in extracted_bands]\n",
    "extracted_values = [[] for _ in range(100)]\n",
    "for index, row in points.iterrows():\n",
    "    point = row.geometry\n",
    "    x, y = point.x, point.y\n",
    "    \n",
    "    # Convert point coordinates to pixel coordinates\n",
    "    px = int((x - tiff_ds.GetGeoTransform()[0]) / tiff_ds.GetGeoTransform()[1])\n",
    "    py = int((y - tiff_ds.GetGeoTransform()[3]) / tiff_ds.GetGeoTransform()[5])\n",
    "    \n",
    "    # Read values from GeoTIFF for each band\n",
    "    for band_num in range(1, 100 + 1):\n",
    "        band = tiff_ds.GetRasterBand(band_num)\n",
    "        value = band.ReadAsArray(px, py, 1, 1)[0][0]\n",
    "        extracted_values[band_num - 1].append(value)\n",
    "\n",
    "extracted_values = pd.DataFrame(np.array(extracted_values)).T\n",
    "extracted_values.columns = extracted_bands\n",
    "\n",
    "tiff_ds = gdal.Open(f\"{data_path}{planet_file}\")\n",
    "num_bands = tiff_ds.RasterCount\n",
    "band_names = []\n",
    "for band_number in range(1, num_bands + 1):\n",
    "    band = tiff_ds.GetRasterBand(band_number)\n",
    "    band_description = band.GetDescription()\n",
    "    band_names.append(band_description)\n",
    "    \n",
    "extracted_values2 = [[] for _ in range(num_bands)]\n",
    "for index, row in points.iterrows():\n",
    "    point = row.geometry\n",
    "    x, y = point.x, point.y\n",
    "    \n",
    "    # Convert point coordinates to pixel coordinates\n",
    "    px = int((x - tiff_ds.GetGeoTransform()[0]) / tiff_ds.GetGeoTransform()[1])\n",
    "    py = int((y - tiff_ds.GetGeoTransform()[3]) / tiff_ds.GetGeoTransform()[5])\n",
    "    \n",
    "    # Read values from GeoTIFF for each band\n",
    "    for band_num in range(1, num_bands + 1):\n",
    "        band = tiff_ds.GetRasterBand(band_num)\n",
    "        value = band.ReadAsArray(px, py, 1, 1)[0][0]\n",
    "        extracted_values2[band_num - 1].append(value)\n",
    "\n",
    "extracted_values2 = pd.DataFrame(np.array(extracted_values2)).T\n",
    "extracted_values2.columns = band_names\n",
    "\n",
    "\n",
    "tiff_ds = gdal.Open(f\"{data_path}{planet_file2}\")\n",
    "num_bands = tiff_ds.RasterCount\n",
    "band_names = []\n",
    "for band_number in range(1, num_bands + 1):\n",
    "    band = tiff_ds.GetRasterBand(band_number)\n",
    "    band_description = band.GetDescription()\n",
    "    band_names.append(band_description)\n",
    "    \n",
    "extracted_values3 = [[] for _ in range(num_bands)]\n",
    "for index, row in points.iterrows():\n",
    "    point = row.geometry\n",
    "    x, y = point.x, point.y\n",
    "    \n",
    "    # Convert point coordinates to pixel coordinates\n",
    "    px = int((x - tiff_ds.GetGeoTransform()[0]) / tiff_ds.GetGeoTransform()[1])\n",
    "    py = int((y - tiff_ds.GetGeoTransform()[3]) / tiff_ds.GetGeoTransform()[5])\n",
    "    \n",
    "    # Read values from GeoTIFF for each band\n",
    "    for band_num in range(1, num_bands + 1):\n",
    "        band = tiff_ds.GetRasterBand(band_num)\n",
    "        value = band.ReadAsArray(px, py, 1, 1)[0][0]\n",
    "        extracted_values3[band_num - 1].append(value)\n",
    "\n",
    "extracted_values3 = pd.DataFrame(np.array(extracted_values3)).T\n",
    "extracted_values3.columns = [f\"{x}_before_noisy\" for x in band_names]\n",
    "\n",
    "final = pd.concat([points,extracted_values,extracted_values2,extracted_values3], axis = 1)\n",
    "final.drop(columns=['geometry'],inplace = True)\n",
    "final.to_csv(f\"{out_path}extracted_samples_reflectance.csv\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c80bdc1c-e383-4529-90c7-3a51cb7624ad",
   "metadata": {},
   "source": [
    "#### Start inter-calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2666f0f6-9203-4de6-b952-63dff002d4af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T18:19:32.629878Z",
     "iopub.status.busy": "2024-10-01T18:19:32.629230Z",
     "iopub.status.idle": "2024-10-01T18:19:32.641997Z",
     "shell.execute_reply": "2024-10-01T18:19:32.640917Z",
     "shell.execute_reply.started": "2024-10-01T18:19:32.629844Z"
    }
   },
   "outputs": [],
   "source": [
    "def rsquared(x, y): \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y) \n",
    "    a = r_value**2\n",
    "    return a\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, nodes):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, nodes)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(nodes, nodes//2)\n",
    "        self.act2 =nn.ReLU()\n",
    "        self.out = nn.Linear(nodes//2, 8)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.manual_seed(0)\n",
    "                init.xavier_normal_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        x =self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "79f5fc28-ae70-452a-8ebf-67e2dee33695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T04:15:53.455721Z",
     "iopub.status.busy": "2024-10-01T04:15:53.455323Z",
     "iopub.status.idle": "2024-10-01T04:19:23.080078Z",
     "shell.execute_reply": "2024-10-01T04:19:23.079296Z",
     "shell.execute_reply.started": "2024-10-01T04:15:53.455687Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████████████████████████████████| 119/119 [01:54<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "spectral_response = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/1_support_materials/Spectral_response_curve_PlanetScope.csv\"\n",
    "df = pd.read_csv(spectral_response)\n",
    "path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "data_path = f\"{path}3_intercalibration_training_data/\"\n",
    "\n",
    "data = pd.read_csv(f\"{data_path}extracted_samples_reflectance.csv\")\n",
    "data = data[(data >= 0).all(axis=1)]\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "emit = data.loc[:,\"381.00558 nm\":\"1118.73682 nm\"]\n",
    "planet = data.loc[:,\"coastal_blue\":\"nir\"]\n",
    "planet2 = data.loc[:,\"coastal_blue_before_noisy\":\"nir_before_noisy\"]\n",
    "emit.to_csv(f'{data_path}0_EMIT_reflectance.csv', index = False)\n",
    "planet.to_csv(f'{data_path}0_Planet_reflectance.csv', index = False)\n",
    "planet2.to_csv(f'{data_path}0_Planet_reflectance_before_noisy.csv', index = False)\n",
    "\n",
    "\n",
    "wl_emit = [float(x.split(' ')[0]) for x in emit.columns]\n",
    "wl_planet = [443, 490, 531, 565, 610, 665, 705, 865]\n",
    "\n",
    "\n",
    "def run_para(data_chunk, df, wvl, fwhm):\n",
    "    var_start = True\n",
    "    for i in range(len(data_chunk)):\n",
    "        hyper_spectra = data_chunk.iloc[i]\n",
    "        response_curve_wl = df[df.columns[0]]\n",
    "        \n",
    "        simulated_refl = []\n",
    "        for kk in range(1,9):\n",
    "            response_curve = df[df.columns[kk]]\n",
    "            refl = compute_weighted_reflectance(response_curve_wl, wvl[:100], hyper_spectra, response_curve, fwhm[:100])\n",
    "            simulated_refl.append(refl)\n",
    "        simulated_refl = pd.DataFrame(np.array(simulated_refl)).T\n",
    "        simulated_refl.columns = [f\"simulated {x}\" for x in planet.columns]\n",
    "        if var_start:\n",
    "            final_simulated_refl = simulated_refl\n",
    "            var_start = False\n",
    "        else:\n",
    "            final_simulated_refl = pd.concat([final_simulated_refl, simulated_refl], axis = 0)\n",
    "    final_simulated_refl.reset_index(drop = True, inplace = True)\n",
    "    return final_simulated_refl\n",
    "\n",
    "chunk_size = 300\n",
    "data_chunks = []\n",
    "for i in range(0, len(emit), chunk_size):\n",
    "    chunk = emit.iloc[i:i+chunk_size]\n",
    "    data_chunks.append(chunk)\n",
    "num_processes = psutil.cpu_count(logical=False)\n",
    "chunk_results = Parallel(n_jobs=num_processes)(delayed(run_para)(data_chunk, df, wvl, fwhm) for data_chunk in tqdm(data_chunks,desc=\"Processing Chunks\"))\n",
    "\n",
    "start_var = True\n",
    "for chunk in chunk_results:\n",
    "    chunk_result = chunk\n",
    "    if start_var:\n",
    "        final = chunk_result\n",
    "        start_var = False\n",
    "    else:\n",
    "        final = pd.concat([final, chunk_result], axis = 0)\n",
    "final.reset_index(drop = True, inplace = True)\n",
    "final.to_csv(f'{data_path}0_Convolved_EMIT_reflectance.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "data_path = f\"{path}3_intercalibration_training_data/\"\n",
    "final_simulated_refl = pd.read_csv(f'{data_path}0_Convolved_EMIT_reflectance.csv')\n",
    "planet = pd.read_csv(f'{data_path}0_Planet_reflectance.csv')\n",
    "\n",
    "X = planet.values\n",
    "y = final_simulated_refl.values\n",
    "\n",
    "## linear model\n",
    "start_t = datetime.datetime.now()\n",
    "print('start linear regression:', start_t)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "joblib.dump(model, f\"{data_path}1_saved_linear_model_for_inter_calibration.pkl\")\n",
    "predict_refl = model.predict(X)\n",
    "\n",
    "predict_refl = pd.DataFrame(predict_refl)\n",
    "predict_refl.columns = [f\"predicted {x}\" for x in planet.columns]\n",
    "predict_refl.to_csv(f'{data_path}1_Corrected_Planet reflectance_linear_regression.csv', index=False)\n",
    "\n",
    "end_t = datetime.datetime.now()\n",
    "elapsed_sec = (end_t - start_t).total_seconds()\n",
    "print('end linear regression:', end_t)\n",
    "print('   total:', elapsed_sec / 60, 'min')\n",
    "print('****************************************************************************************************************')\n",
    "\n",
    "############################################################################################################\n",
    "start_t = datetime.datetime.now()\n",
    "print('start train DNN models:', start_t)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Use {device} to train DNN models\")\n",
    "scaler_x = StandardScaler()\n",
    "X_scale = scaler_x.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scale = scaler_y.fit_transform(y)\n",
    "\n",
    "joblib.dump(scaler_x, f'{data_path}2_scaler_x.pkl')\n",
    "joblib.dump(scaler_y, f'{data_path}2_scaler_y.pkl')\n",
    "\n",
    "# X_tensor, y_tensor = torch.Tensor(X_scale), torch.Tensor(y_scale)\n",
    "X_tensor, y_tensor = torch.Tensor(X_scale).to(device), torch.Tensor(y_scale).to(device)  ####### Use GPU to train\n",
    "\n",
    "param_grid = {'learning_rate': [0.01], 'batch_size': [16, 32, 64], 'nodes': [32, 64, 72]}\n",
    "# param_grid = {'learning_rate':[0.01],'batch_size':[32], 'nodes':[32,40]}             ####### for code testing\n",
    "grid = ParameterGrid(param_grid)\n",
    "loss_fn = nn.L1Loss()\n",
    "best_accuracy = []\n",
    "best_params = []\n",
    "for paras in grid:\n",
    "    learning_rate = paras['learning_rate']\n",
    "    batch_size = paras['batch_size']\n",
    "    nodes = paras['nodes']\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=batch_size, shuffle=True)\n",
    "    model = RegressionModel(X_tensor.size()[1], nodes).to(device)  ####### Use GPU to train\n",
    "    # model = RegressionModel(X_tensor.size()[1], nodes)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.005)\n",
    "    epoch_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.9)\n",
    "    best_mse = pow(10, 10)\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(300):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_scheduler.step()\n",
    "        model.eval()\n",
    "        y_pred = model(X_tensor)\n",
    "        mse = loss_fn(y_pred, y_tensor)\n",
    "        mse = float(mse)\n",
    "        history.append(mse)\n",
    "        if mse < best_mse:\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            best_mse = mse\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_dnn = model(X_tensor)\n",
    "        pred_dnn = pred_dnn.detach().cpu().numpy()\n",
    "        pred_dnn = scaler_y.inverse_transform(pred_dnn)\n",
    "\n",
    "        obs_y = y_tensor.detach().cpu().numpy()\n",
    "        obs_y_raw = scaler_y.inverse_transform(obs_y)\n",
    "        accu = r2_score(pred_dnn, obs_y_raw)\n",
    "\n",
    "        print(\"  \", datetime.datetime.now(), paras, 'R^2:', accu)\n",
    "        best_accuracy.append(accu)\n",
    "        best_params.append(paras)\n",
    "\n",
    "#######obtained  best  parameters\n",
    "new_paras = best_params[best_accuracy.index(max(best_accuracy))]\n",
    "print(\"   bset parameters:\", new_paras)\n",
    "learning_rate = new_paras['learning_rate']\n",
    "batch_size = new_paras['batch_size']\n",
    "nodes = new_paras['nodes']\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=batch_size, shuffle=True)\n",
    "model = RegressionModel(X_tensor.size()[1], nodes).to(device)  ####### Use GPU to train\n",
    "# model = RegressionModel(X_tensor.size()[1], nodes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.005)\n",
    "epoch_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.9)\n",
    "best_mse = pow(10, 10)\n",
    "history = []\n",
    "\n",
    "para_path = f'{data_path}2_saved_DNN_model_for_inter_calibration.pt'\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_scheduler.step()\n",
    "    model.eval()\n",
    "    y_pred = model(X_tensor)\n",
    "    mse = loss_fn(y_pred, y_tensor)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        torch.save(model.state_dict(), para_path)\n",
    "\n",
    "model.load_state_dict(torch.load(para_path))\n",
    "model.eval()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.set_facecolor((0, 0, 0, 0.03))\n",
    "ax.grid(color='gray', linestyle=':', linewidth=0.3)\n",
    "config = {\"font.family\": 'Helvetica'}\n",
    "plt.rcParams.update(config)\n",
    "\n",
    "ax.plot(history, color=\"red\", lw=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('MSE', fontsize=12)\n",
    "plt.savefig(f'figures/1_Epoch for training DNN model.png', dpi=1000, bbox_inches='tight')\n",
    "with torch.no_grad():\n",
    "    pred_dnn = model(X_tensor)\n",
    "    pred_dnn = pred_dnn.detach().cpu().numpy()\n",
    "    pred_dnn = scaler_y.inverse_transform(pred_dnn)\n",
    "\n",
    "pred_dnn = pd.DataFrame(pred_dnn)\n",
    "pred_dnn.columns = [f\"predicted {x}\" for x in planet.columns]\n",
    "pred_dnn.to_csv(f'{data_path}2_Corrected_Planet reflectance_DNN_models.csv', index=False)\n",
    "print('end training DNN model:', end_t)\n",
    "print('   total:', elapsed_sec / 60, 'min')\n",
    "print('****************************************************************************************************************')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81d3d5cf-d33c-48b6-9e7b-b89a5c7a09d2",
   "metadata": {},
   "source": [
    "#### apply trained model to Planet imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3aba529-a217-4145-90f1-84d940f1145a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T18:23:07.847460Z",
     "iopub.status.busy": "2024-10-01T18:23:07.846624Z",
     "iopub.status.idle": "2024-10-01T18:24:50.041541Z",
     "shell.execute_reply": "2024-10-01T18:24:50.041056Z",
     "shell.execute_reply.started": "2024-10-01T18:23:07.847423Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_DNN_models(image_chunk, DNN_model, scaler_x, scaler_y):\n",
    "    image_chunk_reshape = image_chunk.reshape(-1, image_chunk.shape[2])\n",
    "    X = scaler_x.transform(image_chunk_reshape)\n",
    "    X_tensor = torch.Tensor(X)\n",
    "    # model = RegressionModel(X_tensor.size()[1], 32)\n",
    "    model = RegressionModel(X_tensor.size()[1], 72)\n",
    "    model.load_state_dict(torch.load(DNN_model, weights_only=True))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_dnn = model(X_tensor)\n",
    "        pred_dnn = pred_dnn.detach().cpu().numpy()\n",
    "        pred_dnn = scaler_y.inverse_transform(pred_dnn)\n",
    "    pred_dnn = pred_dnn.reshape(image_chunk.shape)\n",
    "    return pred_dnn\n",
    "\n",
    "data_path = \"/Volumes/ChenLab/Fujiang/2_SmallSat_project/3_paired_SHIFT_Planet/1_20230422/\"\n",
    "planet_file = \"PlanetScope_RFL_20230422_clipped_clipped_scaled.tif\"\n",
    "im_data, im_Geotrans, im_proj,rows, cols = read_tif(f\"{data_path}{planet_file}\")\n",
    "\n",
    "dataset = gdal.Open(f\"{data_path}{planet_file}\") \n",
    "num_bands = dataset.RasterCount\n",
    "band_names = []\n",
    "for band_index in range(1, num_bands + 1):\n",
    "    band = dataset.GetRasterBand(band_index)\n",
    "    band_name = band.GetDescription() \n",
    "    band_names.append(band_name)\n",
    "    \n",
    "DNN_model = f\"{data_path}3_intercalibration_training_data/2_saved_DNN_model_for_inter_calibration.pt\"\n",
    "scaler_x =  joblib.load(f\"{data_path}3_intercalibration_training_data/2_scaler_x.pkl\")\n",
    "scaler_y =  joblib.load(f\"{data_path}3_intercalibration_training_data/2_scaler_y.pkl\")\n",
    "\n",
    "corrected_planet = apply_DNN_models(im_data, DNN_model, scaler_x, scaler_y)\n",
    "\n",
    "output_path = f\"{data_path}{planet_file[:-4]}_intersensors_corrected.tif\"\n",
    "array_to_geotiff(corrected_planet, output_path, im_Geotrans, im_proj, band_names=band_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
